import Foundation
import WhisperKit

/// ASREngine implementation backed by WhisperKit (CoreML + Neural Engine).
/// Handles Whisper model download/load and recording via WhisperKit's AudioProcessor.
@MainActor
final class WhisperKitEngine: ASREngine {

    // MARK: - ASREngine conformance

    let isStreaming = false

    private(set) var modelState: ASRModelState = .unloaded
    private(set) var downloadProgress: Double = 0.0
    private(set) var loadingStatusMessage: String = ""

    private var whisperKit: WhisperKit?
    private var lastEnergyUpdateTime: CFAbsoluteTime = 0
    private var sessionStartSampleIndex: Int = 0
    private var sessionStartEnergyIndex: Int = 0

    // Cached values updated from the recording callback
    private(set) var audioSamples: [Float] = []
    private(set) var relativeEnergy: [Float] = []

    private func refreshSessionOffsets(using kit: WhisperKit) {
        sessionStartSampleIndex = kit.audioProcessor.audioSamples.count
        sessionStartEnergyIndex = kit.audioProcessor.relativeEnergy.count
    }

    // MARK: - Model Management

    private func modelFolderKey(for variant: String) -> String {
        "modelFolder_\(variant)"
    }

    func setupModel(_ model: ModelInfo) async throws {
        let logger = InferenceLogger.shared
        guard let variant = model.variant else {
            throw AppError.noModelSelected
        }

        logger.log("[WhisperKitEngine] setupModel: variant=\(variant)")

        // Phase 1: Download
        modelState = .downloading
        downloadProgress = 0.0

        let modelFolderURL: URL
        do {
            modelFolderURL = try await WhisperKit.download(
                variant: variant,
                from: "argmaxinc/whisperkit-coreml",
                progressCallback: { [weak self] progress in
                    Task { @MainActor in
                        self?.downloadProgress = progress.fractionCompleted
                    }
                }
            )
            logger.log("[WhisperKitEngine] Download complete: \(modelFolderURL.path())")
        } catch {
            logger.log("[WhisperKitEngine] Download FAILED: \(error)")
            modelState = .unloaded
            downloadProgress = 0.0
            throw AppError.modelDownloadFailed(underlying: error)
        }

        modelState = .downloaded
        downloadProgress = 1.0

        // Phase 2: Load (CoreML compilation can take 30-60s on first use)
        modelState = .loading
        loadingStatusMessage = "Compiling CoreML model..."
        do {
            logger.log("[WhisperKitEngine] Loading model from: \(modelFolderURL.path())")
            let config = WhisperKitConfig(
                model: variant,
                modelFolder: modelFolderURL.path(),
                computeOptions: ModelComputeOptions(
                    audioEncoderCompute: .cpuAndNeuralEngine,
                    textDecoderCompute: .cpuAndNeuralEngine
                ),
                verbose: true,
                logLevel: .info,
                prewarm: true,
                load: true,
                download: false
            )

            whisperKit = try await WhisperKit(config)
            modelState = .loaded
            loadingStatusMessage = ""
            logger.log("[WhisperKitEngine] Model loaded successfully: \(variant)")

            UserDefaults.standard.set(
                modelFolderURL.path(),
                forKey: modelFolderKey(for: variant)
            )
        } catch {
            logger.log("[WhisperKitEngine] Load FAILED: \(error)")
            modelState = .unloaded
            downloadProgress = 0.0
            loadingStatusMessage = ""
            throw AppError.modelLoadFailed(underlying: error)
        }
    }

    func loadModel(_ model: ModelInfo) async throws {
        guard let variant = model.variant else {
            throw AppError.noModelSelected
        }

        guard let savedFolder = UserDefaults.standard.string(
            forKey: modelFolderKey(for: variant)
        ), FileManager.default.fileExists(atPath: savedFolder) else {
            return
        }

        modelState = .loading
        loadingStatusMessage = "Compiling CoreML model..."

        do {
            let config = WhisperKitConfig(
                model: variant,
                modelFolder: savedFolder,
                computeOptions: ModelComputeOptions(
                    audioEncoderCompute: .cpuAndNeuralEngine,
                    textDecoderCompute: .cpuAndNeuralEngine
                ),
                verbose: true,
                logLevel: .info,
                prewarm: true,
                load: true,
                download: false
            )

            whisperKit = try await WhisperKit(config)
            modelState = .loaded
            loadingStatusMessage = ""
        } catch {
            modelState = .unloaded
            loadingStatusMessage = ""
        }
    }

    func isModelDownloaded(_ model: ModelInfo) -> Bool {
        guard let variant = model.variant else { return false }
        guard let savedFolder = UserDefaults.standard.string(
            forKey: modelFolderKey(for: variant)
        ) else { return false }
        return FileManager.default.fileExists(atPath: savedFolder)
    }

    func unloadModel() async {
        stopRecording()
        await whisperKit?.unloadModels()
        whisperKit = nil
        modelState = .unloaded
        downloadProgress = 0.0
        audioSamples = []
        relativeEnergy = []
        sessionStartSampleIndex = 0
        sessionStartEnergyIndex = 0
    }

    // MARK: - Recording

    func startRecording() async throws {
        guard let whisperKit else { throw AppError.modelNotReady }

        // Explicitly reset cached samples so a restarted session never reuses
        // stale buffers from a previous inference run.
        audioSamples = []
        relativeEnergy = []
        lastEnergyUpdateTime = 0
        refreshSessionOffsets(using: whisperKit)

        let granted = await AudioProcessor.requestRecordPermission()
        guard granted else { throw AppError.microphonePermissionDenied }

        try whisperKit.audioProcessor.startRecordingLive(inputDeviceID: nil) { [weak self] _ in
            Task { @MainActor in
                guard let self else { return }
                let now = CFAbsoluteTimeGetCurrent()
                guard now - self.lastEnergyUpdateTime > 0.1 else { return }
                self.lastEnergyUpdateTime = now
                if let kit = self.whisperKit {
                    let rawEnergy = kit.audioProcessor.relativeEnergy
                    let rawSamples = Array(kit.audioProcessor.audioSamples)
                    let energyStart = min(self.sessionStartEnergyIndex, rawEnergy.count)
                    let sampleStart = min(self.sessionStartSampleIndex, rawSamples.count)
                    self.relativeEnergy = Array(rawEnergy.dropFirst(energyStart))
                    self.audioSamples = Array(rawSamples.dropFirst(sampleStart))
                }
            }
        }
    }

    func stopRecording() {
        whisperKit?.audioProcessor.stopRecording()
        if let kit = whisperKit {
            refreshSessionOffsets(using: kit)
        } else {
            sessionStartSampleIndex = 0
            sessionStartEnergyIndex = 0
        }
        audioSamples = []
        relativeEnergy = []
    }

    // MARK: - Transcription

    func transcribe(audioArray: [Float], options: ASRTranscriptionOptions) async throws -> ASRResult {
        guard let whisperKit else { throw AppError.modelNotReady }

        let workerCount = min(4, ProcessInfo.processInfo.activeProcessorCount)
        let seekClip: [Float] = [0]
        let decodingOptions = DecodingOptions(
            verbose: false,
            task: .transcribe,
            language: options.language,
            temperature: options.temperature,
            temperatureFallbackCount: 3,
            sampleLength: 224,
            usePrefillPrompt: true,
            usePrefillCache: true,
            skipSpecialTokens: true,
            withoutTimestamps: !options.withTimestamps,
            wordTimestamps: options.withTimestamps,
            clipTimestamps: seekClip,
            concurrentWorkerCount: workerCount
        )

        let results = try await whisperKit.transcribe(
            audioArray: audioArray,
            decodeOptions: decodingOptions
        )

        guard let result = results.first else {
            return ASRResult(text: "", segments: [], language: nil)
        }

        let primary = mapResult(result, timeOffset: 0, startingId: 0)
        if !primary.text.isEmpty {
            return primary
        }

        // Fallback for some large variants: decode long clips in overlapping chunks.
        let chunked = try await transcribeChunked(
            whisperKit: whisperKit,
            audioArray: audioArray,
            decodingOptions: decodingOptions
        )
        return chunked.text.isEmpty ? primary : chunked
    }

    private func mapResult(
        _ result: TranscriptionResult,
        timeOffset: Float,
        startingId: Int
    ) -> ASRResult {
        let segments = result.segments.enumerated().map { idx, seg in
            ASRSegment(
                id: startingId + idx,
                text: seg.text,
                start: seg.start + timeOffset,
                end: seg.end + timeOffset
            )
        }

        let segmentText = result.segments.map(\.text)
            .joined(separator: " ")
            .trimmingCharacters(in: .whitespacesAndNewlines)
        let text = segmentText.isEmpty
            ? result.text.trimmingCharacters(in: .whitespacesAndNewlines)
            : segmentText

        return ASRResult(text: text, segments: segments, language: result.language)
    }

    private func transcribeChunked(
        whisperKit: WhisperKit,
        audioArray: [Float],
        decodingOptions: DecodingOptions
    ) async throws -> ASRResult {
        let chunkSize = 16000 * 30
        let overlap = 16000
        var offset = 0
        var nextId = 0
        var combinedText: [String] = []
        var combinedSegments: [ASRSegment] = []
        var detectedLanguage: String?

        while offset < audioArray.count {
            let end = min(offset + chunkSize, audioArray.count)
            let chunk = Array(audioArray[offset..<end])
            let chunkOffsetSeconds = Float(offset) / 16000.0

            let chunkResults = try await whisperKit.transcribe(
                audioArray: chunk,
                decodeOptions: decodingOptions
            )
            if let chunkResult = chunkResults.first {
                let mapped = mapResult(
                    chunkResult,
                    timeOffset: chunkOffsetSeconds,
                    startingId: nextId
                )
                if !mapped.text.isEmpty {
                    combinedText.append(mapped.text)
                    combinedSegments.append(contentsOf: mapped.segments)
                    nextId += mapped.segments.count
                    if detectedLanguage == nil {
                        detectedLanguage = mapped.language
                    }
                }
            }

            if end == audioArray.count { break }
            offset = max(end - overlap, offset + 1)
        }

        return ASRResult(
            text: combinedText.joined(separator: " ").trimmingCharacters(in: .whitespacesAndNewlines),
            segments: combinedSegments,
            language: detectedLanguage
        )
    }
}
